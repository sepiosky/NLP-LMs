{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "WO46fjUseFQU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WO46fjUseFQU",
    "outputId": "5b7eaa82-0bfd-4385-f639-5b6a96cb2679"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intermediate-powell",
   "metadata": {
    "id": "intermediate-powell"
   },
   "outputs": [],
   "source": [
    "import os, itertools, tqdm, codecs, random, pickle\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "#DRIVE_PATH = \"/content/drive/MyDrive/NLP/HW3\"\n",
    "DRIVE_PATH = \"/Users/sepehr/Desktop/Uni/Courses/NLP/HW3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "CmEvSvXy-zYQ",
   "metadata": {
    "id": "CmEvSvXy-zYQ"
   },
   "outputs": [],
   "source": [
    "from importlib.machinery import SourceFileLoader\n",
    "ModelsModule = SourceFileLoader(\"ModelsModule\", DRIVE_PATH+'/Models.py').load_module()\n",
    "DatasetsModule = SourceFileLoader(\"DatasetsModule\", DRIVE_PATH+'/Datasets.py').load_module()\n",
    "from ModelsModule import LSTM, SiameseLSTM, myword2vec\n",
    "from DatasetsModule import MasnaviDataset, RhymeBatchSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-baseball",
   "metadata": {
    "id": "duplicate-baseball"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sealed-jonathan",
   "metadata": {
    "id": "sealed-jonathan"
   },
   "outputs": [],
   "source": [
    "PERSIAN_EMBEDDINGS = \"اأآبپتثجچحخدذرزژسشصضطظعغفقکگلمنوهیئ\"\n",
    "\n",
    "def do_rhyme_words(w1,w2):\n",
    "    return w1.endswith(w2) or w2.endswith(w1) or w2 == w1 or (w1[-2:]==w2[-2:] and w1[-2:]!='ست')\n",
    "\n",
    "def do_rhyme_mesras(m1, m2):\n",
    "    return do_rhyme_words( m1[-1], m2[-1] )\n",
    "\n",
    "def is_masnavi(curr_beyt, next_beyt):\n",
    "    if not do_rhyme_mesras(*curr_beyt):\n",
    "        return False\n",
    "    if (next_beyt is not None) and not do_rhyme_mesras(*next_beyt):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_beyt_rhyme(mesra1, mesra2):\n",
    "    for idx in range(1,min(len(mesra1),len(mesra2))):\n",
    "        if do_rhyme_words(mesra1[-idx], mesra2[-idx]) and mesra1[-idx] != mesra2[-idx]:\n",
    "            return tuple(sorted((mesra1[-idx], mesra2[-idx])))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-assault",
   "metadata": {
    "id": "third-assault"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xYr0D3PIABpN",
   "metadata": {
    "id": "xYr0D3PIABpN"
   },
   "source": [
    "**Skip Bellow Cells And Only Run Loader Cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "selective-clause",
   "metadata": {
    "id": "selective-clause"
   },
   "outputs": [],
   "source": [
    "mesras = {}\n",
    "for filename in os.listdir(DRIVE_PATH+'/Persian_poems_corpus/normalized'):\n",
    "    mesras[filename[:-9]] = list(filter(lambda m:len(m)>2, [x.strip().split() for x in codecs.open(DRIVE_PATH+f'/Persian_poems_corpus/normalized/{filename}','rU','utf-8').readlines()]))\n",
    "with open(DRIVE_PATH+'/datasets/mesras.pickle', 'wb') as f:\n",
    "    pickle.dump(mesras, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "figured-korea",
   "metadata": {
    "id": "figured-korea"
   },
   "outputs": [],
   "source": [
    "masnavis = []\n",
    "for poet,p_mesras in mesras.items():\n",
    "    for idx in range(0,len(p_mesras),2):\n",
    "        if idx+1 >= len(p_mesras):\n",
    "            break\n",
    "        curr_beyt = (p_mesras[idx], p_mesras[idx+1])\n",
    "        next_beyt = None if idx+3 >= len(p_mesras) else (p_mesras[idx+2], p_mesras[idx+3])\n",
    "        if is_masnavi(curr_beyt, next_beyt):\n",
    "            masnavis.append(curr_beyt)\n",
    "with open(DRIVE_PATH+'/datasets/masnavis.pickle', 'wb') as f:\n",
    "    pickle.dump(masnavis, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "demonstrated-penalty",
   "metadata": {
    "id": "demonstrated-penalty"
   },
   "outputs": [],
   "source": [
    "qazals = []\n",
    "for poet,p_mesras in mesras.items():\n",
    "    idx = 0\n",
    "    while idx < len(p_mesras):\n",
    "        if idx+1 >= len(p_mesras):\n",
    "            break\n",
    "        curr_beyt = (p_mesras[idx], p_mesras[idx+1])\n",
    "        idx += 2\n",
    "        if do_rhyme_mesras(*curr_beyt):\n",
    "            qazal = [curr_beyt]\n",
    "            next_beyt = None if idx+1 >= len(p_mesras) else (p_mesras[idx], p_mesras[idx+1])\n",
    "            while (next_beyt is not None) and do_rhyme_mesras(next_beyt[1], curr_beyt[1]):\n",
    "                qazal.append(next_beyt)\n",
    "                idx+=2\n",
    "                next_beyt = None if idx+1 >= len(p_mesras) else (p_mesras[idx], p_mesras[idx+1])\n",
    "            if len(qazal) > 1:\n",
    "                qazals.append(qazal)\n",
    "with open(DRIVE_PATH+'/datasets/qazals.pickle', 'wb') as f:\n",
    "    pickle.dump(qazals, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fifty-median",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "fifty-median"
   },
   "outputs": [],
   "source": [
    "rhymes = []\n",
    "for beyt in masnavis:\n",
    "    rhymes.append( get_beyt_rhyme(*beyt) )\n",
    "for qazal in qazals:\n",
    "    rhymes.append(get_beyt_rhyme(*qazal[0]))\n",
    "   \n",
    "    #option 1\n",
    "    for idx in range(1,len(qazal)):\n",
    "        rhymes.append( get_beyt_rhyme(qazal[0][1], qazal[idx][1]) )\n",
    "    \n",
    "    \n",
    "    # #option2\n",
    "    # for idx in range(0,len(qazal)-1):\n",
    "    #     rhymes.append( get_beyt_rhyme(qazal[idx][1], qazal[idx+1][1]) )\n",
    "    \n",
    "    # option 3\n",
    "#     for idx1 in range(0,len(qazal)):\n",
    "#         for idx2 in range(idx1+1,len(qazal)):\n",
    "#             rhymes.append( get_beyt_rhyme(qazal[idx1][1], qazal[idx2][1]) )\n",
    "rhymes = list(set([r for r in rhymes if (r is not None)]))\n",
    "with open(DRIVE_PATH+'/datasets/rhymes.pickle', 'wb') as f:\n",
    "    pickle.dump(rhymes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZGRWH6hEASlb",
   "metadata": {
    "id": "ZGRWH6hEASlb"
   },
   "source": [
    "## Datasets Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5QiK5DX9i3Em",
   "metadata": {
    "id": "5QiK5DX9i3Em"
   },
   "outputs": [],
   "source": [
    "with open(DRIVE_PATH+'/datasets/mesras.pickle', 'rb') as f:\n",
    "    mesras = pickle.load(f)\n",
    "with open(DRIVE_PATH+'/datasets/masnavis.pickle', 'rb') as f:\n",
    "    masnavis = pickle.load(f)\n",
    "with open(DRIVE_PATH+'/datasets/qazals.pickle', 'rb') as f:\n",
    "    qazals = pickle.load(f)\n",
    "with open(DRIVE_PATH+'/datasets/rhymes.pickle', 'rb') as f:\n",
    "    rhymes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Gb2DXmVB7UE6",
   "metadata": {
    "id": "Gb2DXmVB7UE6"
   },
   "source": [
    "### Pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3xjP4eCHAYDt",
   "metadata": {
    "id": "3xjP4eCHAYDt"
   },
   "outputs": [],
   "source": [
    "#!pip3 install gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "class LiteratureWord2Vec(object):\n",
    "    def __init__(self):\n",
    "        super(LiteratureWord2Vec, self).__init__()\n",
    "        self.corpus = KeyedVectors.load_word2vec_format(DRIVE_PATH+'/datasets/farsi_literature_word2vec_model.txt', binary=False)\n",
    "        self.emb_dim = 100\n",
    "        \n",
    "    def add_new_word(self, w):\n",
    "        emb = self.corpus['ا'].copy()\n",
    "        while self.corpus.most_similar([emb], topn=1)[0][1]>0.5:\n",
    "            emb = np.random.normal(0,1,self.emb_dim)\n",
    "        self.corpus.add_vector(w, emb)\n",
    "        self.corpus.fill_norms(force=True)\n",
    "\n",
    "    def __call__(self, words, pad_to=None):\n",
    "        if isinstance(words, str):\n",
    "            words = [words]\n",
    "        embeddings = []\n",
    "        for word in words:\n",
    "            emb = None\n",
    "            if word in self.corpus:\n",
    "                emb = self.corpus[word]\n",
    "            else:\n",
    "                for i in range(len(word)):\n",
    "                    if word[:i] in self.corpus and word[i:] in self.corpus:\n",
    "                        emb = self.corpus[word[:i]]+self.corpus[word[i:]]\n",
    "            if emb is None:\n",
    "                unk_emb = self.corpus['ا'].copy()\n",
    "                unk_emb[unk_emb!=0]=0 # UNK word embedding - #TODO: its bad for cosine similarity\n",
    "                emb = unk_emb\n",
    "            embeddings.append( torch.Tensor(emb) )\n",
    "            \n",
    "        return torch.vstack(embeddings) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-holly",
   "metadata": {},
   "source": [
    "### Corpus and Word Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incoming-newsletter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/gensim/models/keyedvectors.py:552: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "all_mesras = list(itertools.chain(*mesras.values()))\n",
    "unigrams = list(itertools.chain(*all_mesras))\n",
    "unigrams = list(set([w for w in unigrams]))\n",
    "\n",
    "literatureWord2Vec = LiteratureWord2Vec()\n",
    "\n",
    "special_words = [\"__PAD__\", \"__BOM__\", \"__EOM__\"]\n",
    "for w in special_words:\n",
    "    unigrams.append(w)\n",
    "    literatureWord2Vec.add_new_word(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-selection",
   "metadata": {
    "id": "three-selection"
   },
   "source": [
    "# N-Gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "delayed-percentage",
   "metadata": {
    "cellView": "form",
    "id": "delayed-percentage"
   },
   "outputs": [],
   "source": [
    "#@title Default title text\n",
    "# Modified version of \n",
    "# https://github.com/joshualoehr/ngram-language-model/blob/master/language_model.py\n",
    "from itertools import product\n",
    "import math\n",
    "import nltk\n",
    "\n",
    "\n",
    "class LanguageModel(object):\n",
    "    \"\"\"An n-gram language model trained on a given corpus.\n",
    "    \n",
    "    For a given n and given training corpus, constructs an n-gram language\n",
    "    model for the corpus by:\n",
    "    1. preprocessing the corpus (adding SOS/EOS/UNK tokens)\n",
    "    2. calculating (smoothed) probabilities for each n-gram\n",
    "    Also contains methods for calculating the perplexity of the model\n",
    "    against another corpus, and for generating sentences.\n",
    "    Args:\n",
    "        train_data (list of str): list of sentences comprising the training corpus.\n",
    "        n (int): the order of language model to build (i.e. 1 for unigram, 2 for bigram, etc.).\n",
    "        laplace (int): lambda multiplier to use for laplace smoothing (default 1 for add-1 smoothing).\n",
    "    \"\"\"\n",
    "\n",
    "    SOS = \"__BOM__\"\n",
    "    EOS = \"__EOM__\"\n",
    "    UNK = \"<UNK>\"\n",
    "    \n",
    "    def __init__(self, train_data, n, laplace=1):\n",
    "        self.n = n\n",
    "        self.vocab = dict()\n",
    "        self.laplace = laplace\n",
    "        self.tokens = self.preprocess(train_data, n)\n",
    "        self.vocab  = nltk.FreqDist(self.tokens)\n",
    "        self.model  = self._create_model()\n",
    "        self.masks  = list(reversed(list(product((0,1), repeat=n))))\n",
    "\n",
    "    def _smooth(self):\n",
    "        \"\"\"Apply Laplace smoothing to n-gram frequency distribution.\n",
    "        \n",
    "        Here, n_grams refers to the n-grams of the tokens in the training corpus,\n",
    "        while m_grams refers to the first (n-1) tokens of each n-gram.\n",
    "        Returns:\n",
    "            dict: Mapping of each n-gram (tuple of str) to its Laplace-smoothed \n",
    "            probability (float).\n",
    "        \"\"\"\n",
    "        vocab_size = len(self.vocab)\n",
    "\n",
    "        n_grams = nltk.ngrams(self.tokens, self.n)\n",
    "        n_vocab = nltk.FreqDist(n_grams)\n",
    "\n",
    "        m_grams = nltk.ngrams(self.tokens, self.n-1)\n",
    "        m_vocab = nltk.FreqDist(m_grams)\n",
    "\n",
    "        def smoothed_count(n_gram, n_count):\n",
    "            m_gram = n_gram[:-1]\n",
    "            m_count = m_vocab[m_gram]\n",
    "            return (n_count + self.laplace) / (m_count + self.laplace * vocab_size)\n",
    "\n",
    "        return { n_gram: smoothed_count(n_gram, count) for n_gram, count in n_vocab.items() }\n",
    "\n",
    "    def _create_model(self):\n",
    "        \"\"\"Create a probability distribution for the vocabulary of the training corpus.\n",
    "        \n",
    "        If building a unigram model, the probabilities are simple relative frequencies\n",
    "        of each token with the entire corpus.\n",
    "        Otherwise, the probabilities are Laplace-smoothed relative frequencies.\n",
    "        Returns:\n",
    "            A dict mapping each n-gram (tuple of str) to its probability (float).\n",
    "        \"\"\"\n",
    "        if self.n == 1:\n",
    "            num_tokens = len(self.tokens)\n",
    "            return { (unigram,): count / num_tokens for unigram, count in self.vocab.items() }\n",
    "        else:\n",
    "            return self._smooth()\n",
    "\n",
    "    def _convert_oov(self, ngram):\n",
    "        \"\"\"Convert, if necessary, a given n-gram to one which is known by the model.\n",
    "        Starting with the unmodified ngram, check each possible permutation of the n-gram\n",
    "        with each index of the n-gram containing either the original token or <UNK>. Stop\n",
    "        when the model contains an entry for that permutation.\n",
    "        This is achieved by creating a 'bitmask' for the n-gram tuple, and swapping out\n",
    "        each flagged token for <UNK>. Thus, in the worst case, this function checks 2^n\n",
    "        possible n-grams before returning.\n",
    "        Returns:\n",
    "            The n-gram with <UNK> tokens in certain positions such that the model\n",
    "            contains an entry for it.\n",
    "        \"\"\"\n",
    "        mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token,flag in zip(ngram, bitmask)))\n",
    "\n",
    "        ngram = (ngram,) if type(ngram) is str else ngram\n",
    "        for possible_known in [mask(ngram, bitmask) for bitmask in self.masks]:\n",
    "            if possible_known in self.model:\n",
    "                return possible_known\n",
    "\n",
    "    def perplexity(self, test_data):\n",
    "        \"\"\"Calculate the perplexity of the model against a given test corpus.\n",
    "        \n",
    "        Args:\n",
    "            test_data (list of str): sentences comprising the training corpus.\n",
    "        Returns:\n",
    "            The perplexity of the model as a float.\n",
    "        \n",
    "        \"\"\"\n",
    "        test_tokens = self.preprocess(test_data, self.n)\n",
    "        test_ngrams = nltk.ngrams(test_tokens, self.n)\n",
    "        N = len(test_tokens)\n",
    "\n",
    "        known_ngrams  = [self._convert_oov(ngram) for ngram in test_ngrams]\n",
    "        probabilities = [self.model[ngram] for ngram in known_ngrams]\n",
    "        \n",
    "        for x,y in zip(known_ngrams, probabilities):\n",
    "            print(x,y)\n",
    "        \n",
    "        return math.exp((-1/N) * sum(map(math.log, probabilities)))\n",
    "\n",
    "    def _best_candidate(self, prev, without=[]):\n",
    "        \n",
    "        blacklist  = [LanguageModel.UNK] + without\n",
    "\n",
    "        if len(prev) < self.n:\n",
    "            prev = [LanguageModel.SOS]*(self.n-1)\n",
    "\n",
    "        candidates = list(((ngram[-1],prob) for ngram,prob in self.model.items() if ngram[:-1]==tuple(prev)))\n",
    "\n",
    "        probs = [y for x,y in candidates]\n",
    "        probs = probs/np.sum(probs)\n",
    "        words = [x for x,y in candidates]\n",
    "\n",
    "        idx = np.random.choice(len(words), 1, replace=False, p=probs)[0]\n",
    "        \n",
    "        while words[idx] in blacklist:\n",
    "            idx = np.random.choice(len(words), 1, replace=False, p=probs)[0]\n",
    "        \n",
    "        return (words[idx], probs[idx])\n",
    "         \n",
    "    def generate_sentence(self, input, min_len=12, max_len=24):\n",
    "        #sent, prob = ([LanguageModel.SOS] * (max(1, self.n-1)), 1)\n",
    "        sent, prob, start = input.copy(), 1, True\n",
    "        while sent[-1] != LanguageModel.EOS or start:\n",
    "            start = False\n",
    "            prev = () if self.n == 1 else tuple(sent[-(self.n-1):])\n",
    "            blacklist = sent + ([LanguageModel.EOS,LanguageModel.SOS] if len(sent) < min_len else [])\n",
    "            next_token, next_prob = self._best_candidate(prev, without=blacklist)\n",
    "            sent.append(next_token)\n",
    "            prob *= next_prob\n",
    "\n",
    "            if len(sent) >= max_len:\n",
    "                sent.append(LanguageModel.EOS)\n",
    "\n",
    "        #return (' '.join(sent[(self.n-1):-1]), -1/math.log(prob))\n",
    "        #return (' '.join(sent), -1/math.log(prob))\n",
    "        return ' '.join(sent)\n",
    "    \n",
    "    \n",
    "\n",
    "    def add_sentence_tokens(self, sentences, n):\n",
    "        \"\"\"Wrap each sentence in SOS and EOS tokens.\n",
    "        For n >= 2, n-1 SOS tokens are added, otherwise only one is added.\n",
    "        Args:\n",
    "            sentences (list of str): the sentences to wrap.\n",
    "            n (int): order of the n-gram model which will use these sentences.\n",
    "        Returns:\n",
    "            List of sentences with SOS and EOS tokens wrapped around them.\n",
    "        \"\"\"\n",
    "        sos = ' '.join([LanguageModel.SOS] * (n-1)) if n > 1 else LanguageModel.SOS\n",
    "        return ['{} {} {}'.format(sos, s, LanguageModel.EOS) for s in sentences]\n",
    "\n",
    "    def replace_singletons(self, tokens):\n",
    "        \"\"\"Replace tokens which appear only once in the corpus with <UNK>.\n",
    "\n",
    "        Args:\n",
    "            tokens (list of str): the tokens comprising the corpus.\n",
    "        Returns:\n",
    "            The same list of tokens with each singleton replaced by <UNK>.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(self.vocab) == 0:\n",
    "            self.vocab = nltk.FreqDist(tokens)\n",
    "        return [token if self.vocab[token] > 1 else LanguageModel.UNK for token in tokens]\n",
    "\n",
    "    def preprocess(self, sentences, n):\n",
    "        \"\"\"Add SOS/EOS/UNK tokens to given sentences and tokenize.\n",
    "        Args:\n",
    "            sentences (list of str): the sentences to preprocess.\n",
    "            n (int): order of the n-gram model which will use these sentences.\n",
    "        Returns:\n",
    "            The preprocessed sentences, tokenized by words.\n",
    "        \"\"\"\n",
    "        sentences = self.add_sentence_tokens(sentences, n)\n",
    "        tokens = ' '.join(sentences).split()\n",
    "        tokens = self.replace_singletons(tokens)\n",
    "        return tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ud_6cO0DTosZ",
   "metadata": {
    "id": "ud_6cO0DTosZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FipOmElqTo20",
   "metadata": {
    "id": "FipOmElqTo20"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QnBL0b5lTo9i",
   "metadata": {
    "id": "QnBL0b5lTo9i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "divine-recruitment",
   "metadata": {
    "id": "divine-recruitment"
   },
   "source": [
    "# Encoder - Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hWUPB78t7-R_",
   "metadata": {
    "id": "hWUPB78t7-R_"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_dims, num_layers, dropout, device):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.device = device\n",
    "        self.enc = LSTM(embeddings('ا').size(1), hidden_dims, num_layers, dropout, device)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embeds = torch.stack([self.embeddings(s) for s in input]).to(self.device)\n",
    "        N, L = embeds.size(0), embeds.size(1)\n",
    "        hidden, cell = self.enc.init_hidden(N)\n",
    "        output, hidden, cell = self.enc(embeds, N, hidden, cell)\n",
    "        return output, hidden, cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, corpus, embeddings, hidden_dims, output_dims, num_layers, dropout, device):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.corpus = corpus\n",
    "        self.device = device\n",
    "        self.dec = LSTM(embeddings('ا').size(1), hidden_dims, num_layers, dropout, device)\n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dims,output_dims),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(8192,output_dims),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(output_dims,output_dims),\n",
    "            nn.Softmax(dim=1)\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, target, hidden, cell, teacher_force_prob=1.0):\n",
    "        target_embeds = torch.stack([self.embeddings(s) for s in target]).to(self.device)\n",
    "        N, L = target_embeds.size(0), target_embeds.size(1)\n",
    "        start_token = \"__PAD__\"\n",
    "        dec_input = torch.stack([self.embeddings(start_token)]*N)\n",
    "        dec_hidden, dec_cell = hidden, cell\n",
    "        tf = False if (teacher_force_prob < 1.0 and random.random() < 1.0-teacher_force_prob) else True\n",
    "        pred = []\n",
    "        for l in range(L):\n",
    "            dec_output, dec_hidden, dec_cell = self.dec(dec_input, N, dec_hidden, dec_cell)\n",
    "            dec_pred = self.output_net(dec_output.squeeze(dim=1))\n",
    "            pred.append(dec_pred)\n",
    "            preds_embs =torch.stack([self.embeddings(self.corpus[idx]) for idx in dec_pred.argmax(dim=1)])\n",
    "            dec_input = target_embeds[:,l,:].unsqueeze(dim=1) if tf else preds_embs # detach from history as input\n",
    "        return torch.stack(pred)\n",
    "        \n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, corpus, embeddings, hidden_dims, output_dims, num_layers, dropout, device):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(embeddings, hidden_dims, num_layers, dropout, device)\n",
    "        self.decoder = Decoder(corpus, embeddings, hidden_dims, output_dims, num_layers, dropout, device)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        \n",
    "    def forward(self, input, target, teacher_force_prob=1.0):\n",
    "        _, enc_hidden, enc_cell = self.encoder(input)\n",
    "        return self.decoder(target, enc_hidden, enc_cell, teacher_force_prob).transpose(0,1)\n",
    "    \n",
    "    def get_loss(self, preds, target_indices, mask):\n",
    "        #loss = [self.loss_fn(preds[idx].unsqueeze(dim=0), target_indices[idx].unsqueeze(dim=0))*mask[idx] for idx in range(len(mask))]\n",
    "        loss = self.loss_fn(preds[0:mask,:], target_indices[0:mask].to(self.device))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "juvenile-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_encdec_model(encdec, dataLoader, optim, epochs, device):\n",
    "    encdec.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"=============epoch:{epoch}=============\")\n",
    "        for batch_idx, (mesras, (targets, targets_indices, masks)) in enumerate(dataLoader):\n",
    "            #optim.zero_grad()\n",
    "\n",
    "            mesras = [m.split(\"#\") for m in mesras]\n",
    "            targets = [t.split(\"#\") for t in targets]\n",
    "            targets_indices = targets_indices\n",
    "            preds = encdec(mesras, targets)\n",
    "            loss = 0\n",
    "            for idx, seq in enumerate(preds):\n",
    "                loss += encdec.get_loss(seq, targets_indices[idx], masks[idx])\n",
    "            loss.backward()\n",
    "            if batch_idx%100 ==0:\n",
    "                print(f\"batch {batch_idx} loss: {loss}\")\n",
    "                torch.save(encdec.state_dict(), DRIVE_PATH+f'/encdec_model.pt')\n",
    "            optim.step()\n",
    "    \n",
    "    return encdec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-wayne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============epoch:0=============\n",
      "batch 0 loss: 47.63416290283203\n",
      "batch 1 loss: 47.63416290283203\n",
      "batch 2 loss: 47.63420104980469\n",
      "batch 3 loss: 47.634307861328125\n",
      "batch 4 loss: 47.63398742675781\n",
      "batch 5 loss: 47.63190460205078\n",
      "batch 6 loss: 47.626617431640625\n",
      "batch 7 loss: 47.60655975341797\n",
      "batch 8 loss: 47.533416748046875\n",
      "batch 9 loss: 47.368125915527344\n",
      "batch 10 loss: 47.11322784423828\n",
      "batch 11 loss: 46.976680755615234\n",
      "batch 12 loss: 46.937801361083984\n",
      "batch 13 loss: 46.87327194213867\n",
      "batch 14 loss: 46.930030822753906\n",
      "batch 15 loss: 46.95907211303711\n",
      "batch 16 loss: 46.8367919921875\n",
      "batch 17 loss: 46.84014892578125\n",
      "batch 18 loss: 46.87990951538086\n",
      "batch 19 loss: 46.8219108581543\n",
      "batch 20 loss: 46.851654052734375\n",
      "batch 21 loss: 46.91090393066406\n",
      "batch 22 loss: 46.78603744506836\n",
      "batch 23 loss: 46.89038848876953\n",
      "batch 24 loss: 46.85102844238281\n",
      "batch 25 loss: 46.712928771972656\n",
      "batch 26 loss: 46.81257629394531\n",
      "batch 27 loss: 46.647254943847656\n",
      "batch 28 loss: 46.62928771972656\n",
      "batch 29 loss: 46.626705169677734\n"
     ]
    }
   ],
   "source": [
    "encdec_batch_size = 4\n",
    "encdec_epochs = 5\n",
    "encdec_lr = 0.01\n",
    "encdec_device = 'cpu'\n",
    "\n",
    "masnaviDataset = MasnaviDataset(masnavis, unigrams)\n",
    "masnaviDataLoader = D.DataLoader(masnaviDataset, encdec_batch_size)\n",
    "encdec = EncoderDecoder(corpus=unigrams, embeddings=literatureWord2Vec, hidden_dims=128, output_dims=len(unigrams), num_layers=1, dropout=0.0, device=encdec_device)\n",
    "encdec_optim = torch.optim.Adam(encdec.parameters(), lr=encdec_lr)\n",
    "encdec = train_encdec_model(encdec, masnaviDataLoader, encdec_optim, encdec_epochs, encdec_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-federal",
   "metadata": {
    "id": "valued-federal"
   },
   "source": [
    "## Rhyme Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0LOVvOtn8IDo",
   "metadata": {
    "id": "0LOVvOtn8IDo"
   },
   "source": [
    "### Preparing Rhymes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "GDzWAa6q4SI_",
   "metadata": {
    "id": "GDzWAa6q4SI_"
   },
   "outputs": [],
   "source": [
    "NON_RHYME_DATA_POINT = 1 # TODO bayad 1 bashe vagaran eval ro npratio=0.5 bad mishe (ba 3 result khoob bood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5mYPd0dIa2zQ",
   "metadata": {
    "id": "5mYPd0dIa2zQ"
   },
   "outputs": [],
   "source": [
    "rhymes_dataset = [(w1,w2,1) for (w1,w2) in rhymes]\n",
    "for w1,w2 in rhymes:\n",
    "    for idx in range(NON_RHYME_DATA_POINT):\n",
    "        x = None\n",
    "        while True:\n",
    "            x = random.sample(unigrams,1)[0]\n",
    "            if not do_rhyme_words(w1,x):\n",
    "                break\n",
    "        rhymes_dataset.append( (w1,x,-1) )\n",
    "        while True:\n",
    "            x = random.sample(unigrams,1)[0]\n",
    "            if not do_rhyme_words(w2,x):\n",
    "                break\n",
    "        rhymes_dataset.append( (x,w2,-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brave-development",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brave-development",
    "outputId": "ab28584e-3c4a-4098-edac-bf0dd929fb36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('تعبیر', 'ر', 1),\n",
       " ('اولست', 'منازلم', -1),\n",
       " ('انداختی', 'برجاستی', 1),\n",
       " ('پار', 'یسار', 1),\n",
       " ('طور', 'تاهت', -1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(rhymes_dataset,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "Bn3DOTWxIQOl",
   "metadata": {
    "id": "Bn3DOTWxIQOl"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "train_length, test_length = (int)(0.95*len(rhymes_dataset)), len(rhymes_dataset)-(int)(0.95*len(rhymes_dataset))\n",
    "rhymes_dataset_train, rhymes_dataset_test = torch.utils.data.random_split(rhymes_dataset, [train_length, test_length],torch.Generator().manual_seed(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UxsseV-f9gt4",
   "metadata": {
    "id": "UxsseV-f9gt4"
   },
   "source": [
    "### Rhymes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "collected-portable",
   "metadata": {
    "id": "collected-portable"
   },
   "outputs": [],
   "source": [
    "def run_rhyme_model(rhyme_model, data_loader, batch_size, device, train=True):\n",
    "    print(f\"Running on {device}\")\n",
    "    accs, losses = [], []\n",
    "    for idx, batch in enumerate(data_loader):\n",
    "        rhyme_model.optimizer.zero_grad() # too Result khoobe nabood =)))\n",
    "        \n",
    "        r1 = torch.vstack([myword2vec(batch[0][b]) for b in range(batch_size)]).to(device)\n",
    "        r2 = torch.vstack([myword2vec(batch[1][b]) for b in range(batch_size)]).to(device)\n",
    "        y = torch.Tensor([batch[2][b] for b in range(batch_size)]).to(device)\n",
    "        embedding1, embedding2 = rhyme_model(r1,r2,y)\n",
    "        loss = rhyme_model.get_loss(embedding1.view(batch_size,-1), embedding2.view(batch_size,-1),y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        y_pred = [1 if l<0.3 else -1 for l in loss]\n",
    "        acc = sum([int(y[idx]==y_pred[idx]) for idx in range(len(y))])/len(y)\n",
    "        accs.append(acc)\n",
    "\n",
    "        if train:\n",
    "            loss = loss.mean()\n",
    "            loss.backward()\n",
    "        \n",
    "            rhyme_model.optimizer.step()\n",
    "        \n",
    "            if idx%1000 == 0:\n",
    "                print(f\"Train Loss: {loss}.         Train Acc: {acc}\")\n",
    "                torch.save(rhyme_model.state_dict(), DRIVE_PATH+f'/rhyme_models/rhyme_model{idx}.pt')\n",
    "    return accs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "OXehTuphBY7Q",
   "metadata": {
    "id": "OXehTuphBY7Q"
   },
   "outputs": [],
   "source": [
    "#!rm -rf $DRIVE_PATH/rhyme_models/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SHS8yBqK-Oma",
   "metadata": {
    "id": "SHS8yBqK-Oma"
   },
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "supported-choice",
   "metadata": {
    "id": "supported-choice"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n",
      "Train Loss: 0.5000045299530029.         Train Acc: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m rhymes_train_sampler \u001b[38;5;241m=\u001b[39m RhymeBatchSampler(rhymes_dataset_train, npratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, iterations\u001b[38;5;241m=\u001b[39mtrain_iterations, batch_size\u001b[38;5;241m=\u001b[39mbatch_size) \u001b[38;5;66;03m#npratio=1/(1+NON_RHYME_DATA_POINT)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m rhymes_train_data_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(rhymes_dataset_train, batch_sampler\u001b[38;5;241m=\u001b[39mrhymes_train_sampler)\n\u001b[0;32m---> 11\u001b[0m train_accs, train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mrun_rhyme_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhyme_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhymes_train_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mrun_rhyme_model\u001b[0;34m(rhyme_model, data_loader, batch_size, device, train)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[1;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     rhyme_model\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rhm_batch_size=256\n",
    "rhm_train_iterations = 20000 # approx 5 epoch\n",
    "rhm_device = 'cpu'\n",
    "\n",
    "rhyme_model = SiameseLSTM(embedding_dim=128, hidden_dim=512, num_layers=1, dropout=0.0, learning_rate=0.01, device=rhm_device)\n",
    "#rhyme_model.load_state_dict(torch.load(DRIVE_PATH+'/rhyme_models/rhyme_model94000.pt'))\n",
    "\n",
    "rhymes_train_sampler = RhymeBatchSampler(rhymes_dataset_train, npratio=0.5, iterations=train_iterations, batch_size=rhm_batch_size) #npratio=1/(1+NON_RHYME_DATA_POINT)\n",
    "rhymes_train_data_loader = torch.utils.data.DataLoader(rhymes_dataset_train, batch_sampler=rhymes_train_sampler)\n",
    "\n",
    "train_accs, train_losses = run_rhyme_model(rhyme_model, rhymes_train_data_loader, rhm_batch_size, rhm_device, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "snluGkwo-R0k",
   "metadata": {
    "id": "snluGkwo-R0k"
   },
   "source": [
    "#### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6WG5weRmQUXh",
   "metadata": {
    "id": "6WG5weRmQUXh"
   },
   "outputs": [],
   "source": [
    "rhyme_model = SiameseLSTM(embedding_dim=128, hidden_dim=512, num_layers=1, dropout=0.0, learning_rate=0.01, device=rhm_device)\n",
    "rhymes_test_sampler = RhymeBatchSampler(rhymes_dataset_test, npratio=0.5, iterations=2000, batch_size=rhm_batch_size)\n",
    "rhymes_test_data_loader = torch.utils.data.DataLoader(rhymes_dataset_test, batch_sampler=rhymes_test_sampler)\n",
    "\n",
    "rhyme_model.load_state_dict(torch.load(DRIVE_PATH+'/rhyme_models/rhyme_model9000.pt', map_location=torch.device(rhm_device)))\n",
    "\n",
    "rhyme_model.eval()\n",
    "eval_accs, eval_loss = run_rhyme_model(rhyme_model, rhymes_test_data_loader, rhm_batch_size, rhm_device, train=False)\n",
    "print(f\"Eval Accuracy: {mean(eval_accs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aR7q1zDD1oPI",
   "metadata": {
    "id": "aR7q1zDD1oPI"
   },
   "outputs": [],
   "source": [
    "rhyme_model.predict(\"سازش\",\"بارش\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-objective",
   "metadata": {
    "id": "complete-objective"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-county",
   "metadata": {
    "id": "medium-county"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-convertible",
   "metadata": {
    "id": "filled-convertible"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-calvin",
   "metadata": {
    "id": "clinical-calvin"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-acrylic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
