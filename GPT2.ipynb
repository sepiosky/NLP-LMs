{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjiS_nNQt5fB"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "# !pip install torch\n",
        "# !pip install codecs\n",
        "\n",
        "\n",
        "import torch\n",
        "import itertools, random, pickle, codecs\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as D\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5a_c8VJl7m9T"
      },
      "outputs": [],
      "source": [
        "from transformers import(\n",
        "    AutoTokenizer,\n",
        "    TextDataset,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    AutoModelWithLMHead,\n",
        "    AutoConfig,\n",
        "    GPT2Config,\n",
        "     pipeline,\n",
        "      GPT2LMHeadModel,\n",
        "      GPT2Tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWAtluqPg4rn",
        "outputId": "a807acd4-6b8f-46da-ef80-c83f67faeafd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "spZN39ZuVcMg"
      },
      "outputs": [],
      "source": [
        "DRIVE_PATH = \"/content/drive/MyDrive/NLP/HW3\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.machinery import SourceFileLoader\n",
        "ModelsModule = SourceFileLoader(\"ModelsModule\", DRIVE_PATH+'/Models.py').load_module()\n",
        "\n",
        "from ModelsModule import SiameseLSTM\n"
      ],
      "metadata": {
        "id": "biUg6YYVlOvq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evQIFraML0KR"
      },
      "outputs": [],
      "source": [
        "#download pretrained gpt2 model\n",
        "!wget \"https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/pytorch_model.bin\" -P /content/gpt2/\n",
        "!wget \"https://huggingface.co/bolbolzaban/gpt2-persian/resolve/main/tokenizer.json\" -P /content/gpt2/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTmrpavf6rL1"
      },
      "outputs": [],
      "source": [
        "!cp -r \"/content/gpt2\" DRIVE_PATH+'/gpt2'   #save model in drive for future use, or not :D "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**"
      ],
      "metadata": {
        "id": "XPcW5QE3BleG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "z4GEAng6rCEU"
      },
      "outputs": [],
      "source": [
        "mesras = list(filter(lambda m:len(m)>2, \\\n",
        "                     [x.strip().split() for x in codecs.open(DRIVE_PATH+f'/Persian_poems_corpus/normalized/ferdousi_norm.txt','rU','utf-8').readlines()]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "PN2wUTg5kaCD"
      },
      "outputs": [],
      "source": [
        "rhyming_beyts = []\n",
        "non_rhyming_beyts = []\n",
        "\n",
        "for idx in range(0, len(mesras)-1, 2):\n",
        "  rhyming_beyts.append((' '.join(mesras[idx]), ' '.join(mesras[idx+1])))\n",
        "\n",
        "\n",
        "dataset = []\n",
        "for beyt in rhyming_beyts:\n",
        "  dataset.append('[BOM] ' + (beyt[0])   + ' [BOM] '  + (beyt[1]) + ' [EOS]') \n",
        "# for beyt in non_rhyming_beyts:\n",
        "#   dataset.append('[CLS]ندارد[CLS]' + '[BOM]' + (beyt[0])  +  '[BOM]' + (beyt[1]) + '[EOS]' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MwY3srMLINfs",
        "outputId": "6bafc2cc-7c9b-41af-8006-402334682683"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are 44643 samples for training, and 4961 samples for validation testing'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 281
        }
      ],
      "source": [
        "random.shuffle(dataset)\n",
        "train_dataset = dataset[:train_size]\n",
        "val_dataset = dataset[:val_size]\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "f'There are {len(train_dataset)} samples for training, and {len(val_dataset)} samples for validation testing'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {
        "id": "lplxA4inIReU"
      },
      "outputs": [],
      "source": [
        "\n",
        "with open(DRIVE_PATH+\"/train.txt\", \"w\") as output:\n",
        "      for row in train_dataset:\n",
        "            s = str(row)\n",
        "            output.write(s+'\\n')\n",
        "\n",
        "with open(DRIVE_PATH+\"/test.txt\", \"w\") as output:\n",
        "    for row in val_dataset:\n",
        "            s = str(row)\n",
        "            output.write(s+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ho5oIfRjEHhh"
      },
      "outputs": [],
      "source": [
        "# model_name = \"HooshvareLab/gpt2-fa\"\n",
        "model_name = \"bolbolzaban/gpt2-persian\"\n",
        "model_path = DRIVE_PATH+'/gpt2' # or /content/gpt2\n",
        "trained_model_path = DRIVE_PATH+'/model'\n",
        "train_path =  DRIVE_PATH+\"/train.txt\"\n",
        "test_path =  DRIVE_PATH+\"/test.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC2uTcKv3TAj"
      },
      "source": [
        "**Training Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdnnYyAeEt5H"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# special_tokens_dict = { 'additional_special_tokens': [ '[EOM]']}\n",
        "\n",
        "# num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "# config = AutoConfig.from_pretrained(\n",
        "#     model_name)\n",
        "\n",
        "# tokenizer.save_pretrained(\"/content/gpt2/\")\n",
        "# config.save_pretrained(\"/content/gpt2/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQ8H7nM7-kxS"
      },
      "outputs": [],
      "source": [
        "configuration = GPT2Config.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelWithLMHead.from_pretrained(model_path , config = configuration)\n",
        "model.resize_token_embeddings(len(tokenizer))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vlCumMh7wgKA"
      },
      "outputs": [],
      "source": [
        "def freeze_lower_layers():\n",
        "    for param in model.base_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    for param in (\n",
        "        model.base_model.h[23].parameters() or model.base_model.h[22].parameters()\n",
        "    ):\n",
        "        param.requires_grad = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eNUBWtH73sd7"
      },
      "outputs": [],
      "source": [
        "freeze_lower_layers()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_-RslFEdSkx6"
      },
      "outputs": [],
      "source": [
        "def load_dataset(train_path, test_path, tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "        tokenizer=tokenizer, file_path=train_path, block_size=256\n",
        "    )\n",
        "\n",
        "    test_dataset = TextDataset(tokenizer=tokenizer, file_path=test_path, block_size=256)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,\n",
        "    )\n",
        "\n",
        "    return train_dataset, test_dataset, data_collator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzK3CaH6ho2S"
      },
      "outputs": [],
      "source": [
        "train_dataset, test_dataset, data_collator = load_dataset(\n",
        "    train_path, test_path, tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zgo4J21IaYc",
        "outputId": "c262a6c5-ca5d-411d-c36f-8ff28b83c8e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# epochs = 5\n",
        "# sample_every = 500\n",
        "# learning_rate = 3e-5\n",
        "# eps = 1e-8\n",
        "# warmup_steps = 500\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=DRIVE_PATH+\"/model\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=12,\n",
        "    per_device_eval_batch_size=12,\n",
        "    eval_steps=1000,\n",
        "    save_steps=1000,\n",
        "    warmup_steps=500,\n",
        ")\n",
        "training_args.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGhN0sfXJ22C"
      },
      "outputs": [],
      "source": [
        "CUDA_LAUNCH_BLOCKING=1\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# save\n",
        "trainer.save_model()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**"
      ],
      "metadata": {
        "id": "wDUCiDOsDmfT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjDUA3K-3wTi"
      },
      "outputs": [],
      "source": [
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(trained_model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "EqvXgYhktO29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "B93GgGj_Pcrh"
      },
      "outputs": [],
      "source": [
        "model1 = GPT2LMHeadModel.from_pretrained(trained_model_path + '/checkpoint-eom')\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(model_name)\n",
        "special_tokens_dict = { 'additional_special_tokens': [ '[EOM]']}\n",
        "\n",
        "num_added_tokens = tokenizer1.add_special_tokens(special_tokens_dict)\n",
        "model1.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gqHNy3tM7DEF"
      },
      "outputs": [],
      "source": [
        "input = \"[BOM] بی تو روزی از آن کوچه گذشتم [BOM]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EPpXP4fF4TSP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87fdc1a9-1348-4cd1-98c3-2951542d3ea8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:9 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': '[BOM] بی تو روزی از آن کوچه گذشتم [BOM] که ز یک کوچه ندیدیم نشانت'},\n",
              " {'generated_text': '[BOM] بی تو روزی از آن کوچه گذشتم [BOM] که به دیدار تو از دور رسیدم'},\n",
              " {'generated_text': '[BOM] بی تو روزی از آن کوچه گذشتم [BOM] بی تو شب بر سر آن کوچه گذشتم'},\n",
              " {'generated_text': '[BOM] بی تو روزی از آن کوچه گذشتم [BOM] که مرا خانه به دوشان گذر افتاد'},\n",
              " {'generated_text': '[BOM] بی تو روزی از آن کوچه گذشتم [BOM] که به اندازه یک عمر نشستم'}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        " generator(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsxQAZ5wLNm8",
        "outputId": "0910862c-4f04-4fe8-b0f3-22cdd2bc2f6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SiameseLSTM(\n",
              "  (char_embedding): Embedding(36, 128)\n",
              "  (lstm): LSTM(\n",
              "    (lstm): LSTM(128, 512, batch_first=True)\n",
              "  )\n",
              "  (loss_fn): CosineEmbeddingLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "   device = torch.device(\"cuda:0\")\n",
        "\n",
        "rhyme_model = SiameseLSTM(embedding_dim=128, hidden_dim=512, num_layers=1, dropout=0.0, learning_rate=0.01, device=device)\n",
        "\n",
        "rhyme_model.load_state_dict(torch.load(DRIVE_PATH+'/rhyme_models/rhyme_model19000.pt', map_location=torch.device(device)))\n",
        "\n",
        "rhyme_model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "7ofeenL2QtK8"
      },
      "outputs": [],
      "source": [
        "def do_rhyme_words(w1,w2):\n",
        "    return (rhyme_model.predict(w1, w2) > 0.93)\n",
        "\n",
        "def do_rhyme_mesras(m1, m2):\n",
        "    if m1[-1] == m2[-1]:\n",
        "      return do_rhyme_words( m1[-2], m2[-2] )\n",
        "    else:\n",
        "      return do_rhyme_words( m1[-1], m2[-1] )\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def generator(model, tokenizer, input, rhyme = True, max_length=20,  num_return_sequences=10):\n",
        "    prompt = f\"[BOM] {input} [BOM]\"\n",
        "\n",
        "    generator_pipline = pipeline(\"text-generation\", model, tokenizer=tokenizer, do_sample = True,\\\n",
        "                    top_k = 50, top_p = 0.95, \\\n",
        "                    return_full_text = False, \\\n",
        "                    config={\"max_length\": max_length}, num_return_sequences = num_return_sequences)\n",
        "\n",
        "    rhyming = []\n",
        "    non_rhyming = []\n",
        "  \n",
        "    repeat = (rhyme and len(rhyming) == 0) or ((not rhyme) and len(non_rhyming) == 0)\n",
        "\n",
        "    while(repeat):\n",
        "\n",
        "      gen_outputs = generator_pipline(prompt)\n",
        "\n",
        "      for  output in gen_outputs:\n",
        "        out = output['generated_text']\n",
        "        # print(out)\n",
        "        out = out.replace(\"[BOM]\", \" \")\n",
        "        if do_rhyme_mesras(input.strip().split() , out.strip().split() ):\n",
        "              rhyming.append(out)              \n",
        "        else:\n",
        "              non_rhyming.append(out)\n",
        "      \n",
        "      repeat = (rhyme and len(rhyming) == 0) or (not rhyme and len(non_rhyming) == 0)\n",
        "\n",
        "\n",
        "    print(f'Input: {input} \\n')\n",
        "    final_outputs = [rhyming if rhyme else non_rhyming]\n",
        "    print(f'Generated outputs:\\n')\n",
        "    print(*final_outputs, sep = '\\n')\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "2riwhNk8rDYk"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = 'بی تو روزی از آن کوچه گذشتم'\n",
        "generator(model = model, tokenizer = tokenizer, input = input, rhyme = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWI7FeM0yRFT",
        "outputId": "76a5152d-d762-4ffd-adcc-1ef21b39fdb2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:9 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: بی تو روزی از آن کوچه گذشتم \n",
            "\n",
            "Generated outputs:\n",
            "\n",
            "[' که دگر بار نیامد ز تو بویم', ' از تو ای عشق تو ای باده شنیدم', ' جز به دیدار تو ای دوست ندیدم', ' وز سر کوی تو رفتم', ' که به جز سایه دیوار ندیدم']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = 'بی تو روزی از آن کوچه گذشتم'\n",
        "generator(model = model, tokenizer = tokenizer, input = input, rhyme = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLSkIINs4ISR",
        "outputId": "64c199f8-af43-4ed2-fa3a-1404d63a5426"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:9 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: بی تو روزی از آن کوچه گذشتم \n",
            "\n",
            "Generated outputs:\n",
            "\n",
            "[' که ز اندازه گذشتند و نجستند مرا', ' که مرا یاد نکردی چو گذشتی', ' رفت آن شب که ز تیمار تو دل بر دارم', ' تا نگویی که که این باغ تو را کیست', ' همچو سروی که بود سایه دیوار', ' به خیال تو که دل بسته به مویی', ' که ترا در بر و بی برگ و نوایی باشد']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = 'گاه گویند دریا خاک شد'\n",
        "generator(model = model, tokenizer = tokenizer, input = input, rhyme = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKLq8Rvr4mKI",
        "outputId": "1dde61fb-23e8-4b91-d3f0-99380f0ba526"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:9 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: گاه گویند دریا خاک شد \n",
            "\n",
            "Generated outputs:\n",
            "\n",
            "[' گه بگویند کشتی چاک شد', ' گاه گویند با خاک آمد', ' گاه گویند آتشش خاشاک شد', ' گه گویند گردون چاک شد']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "tlTppz9V3THs"
      },
      "outputs": [],
      "source": [
        "# import re\n",
        "# def generator(prompt, max_length=20,  num_return_sequences=10):\n",
        "#     prompt = f\"[BOM] {prompt} [BOM] \"\n",
        "#     print(prompt)\n",
        "#     generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "#     # generated = generated.to(device)\n",
        "\n",
        "#     decoded_outputs = model.generate(\n",
        "#         generated,\n",
        "#         do_sample=True,\n",
        "#         top_k=50,\n",
        "#         max_length=max_length, \n",
        "#         top_p=0.95,\n",
        "#         num_return_sequences=num_return_sequences,\n",
        "#         output_scores=True\n",
        "#     )\n",
        "#     print(decoded_outputs)\n",
        "#     gen_sequences = decoded_outputs.sequences[:, generated.shape[-1]:]\n",
        "\n",
        "\n",
        "#     probs = torch.stack(decoded_outputs.scores, dim=1).softmax(-1)  # -> shape [3, 15, vocab_size]\n",
        "\n",
        "#     gen_probs = torch.gather(probs, 2, gen_sequences[:, :, None]).squeeze(-1)\n",
        "#     normed_gen_probs = gen_probs / gen_probs.sum(0)\n",
        "\n",
        "#     unique_normed_prob_per_sequence = normed_gen_probs.prod(-1)\n",
        "\n",
        "\n",
        "#     for i, output in enumerate(gen_sequences):\n",
        "#         o = tokenizer.decode(output, skip_special_tokens=True)\n",
        "#         o = o.replace(\"[BOM]\", \" \")\n",
        "\n",
        "\n",
        "#         print(o)\n",
        "#         print(f'probability:{unique_normed_prob_per_sequence[i]}')\n",
        "#         print('-' * 100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "GPT2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}